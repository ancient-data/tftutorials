{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Athenaeus\n",
    "\n",
    "Without a TF-app, we use the more\n",
    "[primitive way](https://annotation.github.io/text-fabric/Api/Fabric/)\n",
    "to load a TF data set.\n",
    "\n",
    "Text-Fabric will not go online, but look on the local computer where we tell it to look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location\n",
    "\n",
    "We specify where on our system the data can be found.\n",
    "\n",
    "Most TF users have a directory `github` in their home directory.\n",
    "That directory is organized as in GitHub itself: first by org/user, then by repo.\n",
    "\n",
    "This way, we can borrow each other's code without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = os.path.expanduser('~/github')\n",
    "ORG = 'pthu'\n",
    "REPO = 'patristics'\n",
    "LOCATION1 = 'tf/1.1/athenaeus/Athenaeus/'\n",
    "LOCATION2 = 'Deipnosophistae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split of the relative path in `LOCATION1` and `LOCATION2` is arbitrary.\n",
    "In some scenarios it is handy to split a data set in a base set and a collection modules.\n",
    "\n",
    "Now we are going to call TF. We only ask it to search through the specified directories and\n",
    "read the metadata section of all `.tf` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.8.8\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "25 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations=f'{BASE}/{ORG}/{REPO}/{LOCATION1}', modules=LOCATION2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data and receive an API for it.\n",
    "\n",
    "We want all loadable features, so we ask which they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfInitiate():\n",
    "    api = TF.load('', silent=False)\n",
    "    allFeatures = TF.explore(silent=True, show=True)\n",
    "    loadableFeatures = allFeatures['nodes'] + allFeatures['edges']\n",
    "    TF.load(loadableFeatures, add=True, silent=True)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time will see a precomputation step (levels, order, rank, ...)\n",
    "\n",
    "A lot of data is precomputed and stored in a hidden `.tf` directory, so that it loads quicker next time.\n",
    "The API relies on these precomputed data for efficientcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.11s T otype                from /Users/dirk/github/pthu/patristics/tf/1.1/athenaeus/Athenaeus//Deipnosophistae\n",
      "   |     1.80s T oslots               from /Users/dirk/github/pthu/patristics/tf/1.1/athenaeus/Athenaeus//Deipnosophistae\n",
      "   |     0.68s T norm                 from /Users/dirk/github/pthu/patristics/tf/1.1/athenaeus/Athenaeus//Deipnosophistae\n",
      "   |     0.67s T main                 from /Users/dirk/github/pthu/patristics/tf/1.1/athenaeus/Athenaeus//Deipnosophistae\n",
      "   |     0.00s T book                 from /Users/dirk/github/pthu/patristics/tf/1.1/athenaeus/Athenaeus//Deipnosophistae\n",
      "   |     0.66s T orig                 from /Users/dirk/github/pthu/patristics/tf/1.1/athenaeus/Athenaeus//Deipnosophistae\n",
      "   |     0.65s T plain                from /Users/dirk/github/pthu/patristics/tf/1.1/athenaeus/Athenaeus//Deipnosophistae\n",
      "   |     0.60s T beta_plain           from /Users/dirk/github/pthu/patristics/tf/1.1/athenaeus/Athenaeus//Deipnosophistae\n",
      "   |     0.00s T chapter              from /Users/dirk/github/pthu/patristics/tf/1.1/athenaeus/Athenaeus//Deipnosophistae\n",
      "   |     0.65s T lemma                from /Users/dirk/github/pthu/patristics/tf/1.1/athenaeus/Athenaeus//Deipnosophistae\n",
      "   |     0.00s T _book                from /Users/dirk/github/pthu/patristics/tf/1.1/athenaeus/Athenaeus//Deipnosophistae\n",
      "   |      |     0.03s C __levels__           from otype, oslots, otext\n",
      "   |      |     4.26s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.11s C __rank__             from otype, __order__\n",
      "   |      |     4.88s C __levUp__            from otype, oslots, __rank__\n",
      "   |      |     0.33s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     1.04s C __boundary__         from otype, oslots, __rank__\n",
      "   |      |     0.01s C __sections__         from otype, oslots, otext, __levUp__, __levels__, _book, book, chapter\n",
      "   |      |     0.02s C __structure__        from otype, oslots, otext, __rank__, __levUp__, _book, book, chapter\n",
      "    17s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = tfInitiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to make the functions of the TF API available at the top level in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `docs` is a list of function names that have been inserted in the global namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Computed', 'computed-data', ('C Computed', 'Call AllComputeds', 'Cs ComputedString'))\n",
      "('Features', 'edge-features', ('E Edge', 'Eall AllEdges', 'Es EdgeString'))\n",
      "('Fabric', 'loading', ('ensureLoaded', 'TF', 'ignored', 'loadLog'))\n",
      "('Locality', 'locality', ('L Locality',))\n",
      "('Misc', 'messaging', ('cache', 'error', 'indent', 'info', 'isSilent', 'reset', 'setSilent', 'silentOff', 'silentOn', 'warning'))\n",
      "('Nodes', 'navigating-nodes', ('N Nodes', 'sortKey', 'sortKeyTuple', 'otypeRank', 'sortNodes'))\n",
      "('Features', 'node-features', ('F Feature', 'Fall AllFeatures', 'Fs FeatureString'))\n",
      "('Search', 'search', ('S Search',))\n",
      "('Text', 'text', ('T Text',))\n"
     ]
    }
   ],
   "source": [
    "for member in docs:\n",
    "  print(member)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can study as in the tutorials for DSS and Oldbabylonian:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list all loaded features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_book',\n",
       " '_sentence',\n",
       " 'add',\n",
       " 'beta_plain',\n",
       " 'bibl',\n",
       " 'book',\n",
       " 'chapter',\n",
       " 'cit',\n",
       " 'head',\n",
       " 'hi',\n",
       " 'l',\n",
       " 'lemma',\n",
       " 'main',\n",
       " 'norm',\n",
       " 'num',\n",
       " 'orig',\n",
       " 'otype',\n",
       " 'p',\n",
       " 'pb',\n",
       " 'plain',\n",
       " 'post',\n",
       " 'pre',\n",
       " 'quote']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:17:43.894153Z",
     "start_time": "2018-05-18T09:17:43.597128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Counting nodes ...\n",
      "  0.05s 303854 nodes\n"
     ]
    }
   ],
   "source": [
    "indent(reset=True)\n",
    "info('Counting nodes ...')\n",
    "\n",
    "i = 0\n",
    "for n in N(): i += 1\n",
    "\n",
    "info('{} nodes'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:17:47.820323Z",
     "start_time": "2018-05-18T09:17:47.812328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.slotType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:17:49.922863Z",
     "start_time": "2018-05-18T09:17:49.916078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('_book',\n",
       " 'head',\n",
       " 'book',\n",
       " 'hi',\n",
       " 'cit',\n",
       " 'num',\n",
       " 'add',\n",
       " 'chapter',\n",
       " 'pb',\n",
       " 'p',\n",
       " 'quote',\n",
       " 'bibl',\n",
       " 'l',\n",
       " '_sentence',\n",
       " 'word')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:17:51.782779Z",
     "start_time": "2018-05-18T09:17:51.774167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('_book', 265642.0, 265643, 265643),\n",
       " ('head', 265642.0, 285988, 285988),\n",
       " ('book', 17709.466666666667, 284478, 284492),\n",
       " ('hi', 3120.6153846153848, 285989, 286066),\n",
       " ('cit', 1588.9880239520958, 285821, 285987),\n",
       " ('num', 951.5309090909091, 297334, 297608),\n",
       " ('add', 336.5678073510773, 278425, 279213),\n",
       " ('chapter', 200.03162650602408, 284493, 285820),\n",
       " ('pb', 171.50484183344093, 299180, 300728),\n",
       " ('p', 169.09993634627625, 297609, 299179),\n",
       " ('quote', 84.89507357645553, 300729, 303854),\n",
       " ('bibl', 51.098974164133736, 279214, 284477),\n",
       " ('l', 23.543622969734624, 286067, 297333),\n",
       " ('_sentence', 20.788122995070808, 265644, 278424),\n",
       " ('word', 1, 1, 265642))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.levels.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 _books\n",
      "      1 heads\n",
      "     15 books\n",
      "     78 his\n",
      "    167 cits\n",
      "    275 nums\n",
      "    789 adds\n",
      "   1328 chapters\n",
      "   1549 pbs\n",
      "   1571 ps\n",
      "   3126 quotes\n",
      "   5264 bibls\n",
      "  11267 ls\n",
      "  12781 _sentences\n",
      " 265642 words\n"
     ]
    }
   ],
   "source": [
    "for (typ, av, start, end) in C.levels.data:\n",
    "  print(f'{end - start + 1:>7} {typ}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no linguistic features, as far as I can see, but there is `lemma`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word matters\n",
    "\n",
    "## Top 20 frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24736 ὁ,ὅς\n",
      "12995 καί\n",
      "11725 δέ\n",
      " 5668 ἒ\n",
      " 3139 φημί\n",
      " 2916 οὗτος\n",
      " 2696 αὐτός\n",
      " 2558 ἐγώ\n",
      " 2457 τις,ὁ,τίς,ὅς\n",
      " 2369 εἰμί\n",
      " 2056 οὐ\n",
      " 2007 γάρ\n",
      " 1932 τίς,τῷ,ὅς,ὁ,τις\n",
      " 1856 ὁ,τίς,ὅς\n",
      " 1807 ὡς,ὅς\n",
      " 1742 περί\n",
      " 1597 τε,σύ,τεός,τις\n",
      " 1389 ἐπί\n",
      " 1322 λέγω1,λέγω\n",
      " 1312 εἰς,εἶμι,εἰμί\n"
     ]
    }
   ],
   "source": [
    "for (w, amount) in F.lemma.freqList('word')[0:20]:\n",
    "  print(f'{amount:>5} {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hapaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11194"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hapaxes1 = sorted(lx for (lx, amount) in F.lemma.freqList('word') if amount == 1)\n",
    "len(hapaxes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*isgreek\n",
      "*p\n",
      "*ʼαγκυλητους\n",
      "*ʼαδεσθαι\n",
      "*ʼαδυφωνον\n",
      "*ʼακουσομεθα\n",
      "*ʼαναξαρχον\n",
      "*ʼανδρομαχον\n",
      "*ʼανθος\n",
      "*ʼαντιφωντος\n",
      "*ʼαπο\n",
      "*ʼαποδιδωσι\n",
      "*ʼαρπασθηναι\n",
      "*ʼαφι\n",
      "*ʼβρενθιν\n",
      "*ʼβυζαντιους\n",
      "*ʼγενη\n",
      "*ʼγλαυκου\n",
      "*ʼγραφει\n",
      "*ʼγραφων\n"
     ]
    }
   ],
   "source": [
    "for lx in hapaxes1[0:20]:\n",
    "  print(lx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small occurrence base\n",
    "\n",
    "The occurrence base of a word are the books in which the word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s compiling occurrence base ...\n",
      "  0.22s done\n",
      "  0.22s 23448 entries\n"
     ]
    }
   ],
   "source": [
    "occurrenceBase = collections.defaultdict(set)\n",
    "\n",
    "indent(reset=True)\n",
    "info('compiling occurrence base ...')\n",
    "for s in F.otype.s('book'):\n",
    "  book = F.book.v(s)\n",
    "  for w in L.d(s, otype='word'):\n",
    "    occurrenceBase[F.lemma.v(w)].add(book)\n",
    "info('done')\n",
    "info(f'{len(occurrenceBase)} entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of how many words have how big occurrence bases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books    1 : 12899 words\n",
      "books    2 :  3627 words\n",
      "books    3 :  1879 words\n",
      "books    4 :  1179 words\n",
      "books    5 :   781 words\n",
      "books    6 :   571 words\n",
      "books    7 :   435 words\n",
      "books    8 :   375 words\n",
      "books   15 :   347 words\n",
      "books   10 :   294 words\n",
      "...\n",
      "books    6 :   571 words\n",
      "books    7 :   435 words\n",
      "books    8 :   375 words\n",
      "books   15 :   347 words\n",
      "books   10 :   294 words\n",
      "books    9 :   288 words\n",
      "books   11 :   218 words\n",
      "books   12 :   205 words\n",
      "books   14 :   176 words\n",
      "books   13 :   174 words\n"
     ]
    }
   ],
   "source": [
    "occurrenceSize = collections.Counter()\n",
    "\n",
    "for (w, books) in occurrenceBase.items():\n",
    "  occurrenceSize[len(books)] += 1\n",
    "  \n",
    "occurrenceSize = sorted(\n",
    "  occurrenceSize.items(),\n",
    "  key=lambda x: (-x[1], x[0]),\n",
    ")\n",
    "\n",
    "for (size, amount) in occurrenceSize[0:10]:\n",
    "  print(f'books {size:>4} : {amount:>5} words')\n",
    "print('...')\n",
    "for (size, amount) in occurrenceSize[-10:]:\n",
    "  print(f'books {size:>4} : {amount:>5} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give the predicate *private* to those words whose occurrence base is a single book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12899"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privates = {w for (w, base) in occurrenceBase.items() if len(base) == 1}\n",
    "len(privates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peculiarity of books\n",
    "\n",
    "As a final exercise with books, lets make a list of all books, and show their\n",
    "\n",
    "* total number of words\n",
    "* number of private words\n",
    "* the percentage of private words: a measure of the peculiarity of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:18:52.143337Z",
     "start_time": "2018-05-18T09:18:52.130385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found    0 empty books\n",
      "Found    0 ordinary books (i.e. without private words)\n"
     ]
    }
   ],
   "source": [
    "bookList = []\n",
    "\n",
    "empty = set()\n",
    "ordinary = set()\n",
    "\n",
    "for d in F.otype.s('book'):\n",
    "  book = F.book.v(d)\n",
    "  words = {F.lemma.v(w) for w in L.d(d, otype='word')}\n",
    "  a = len(words)\n",
    "  if not a:\n",
    "    empty.add(book)\n",
    "    continue\n",
    "  o = len({w for w in words if w in privates})\n",
    "  if not o:\n",
    "    ordinary.add(book)\n",
    "    continue\n",
    "  p = 100 * o / a\n",
    "  bookList.append((book, a, o, p))\n",
    "\n",
    "bookList = sorted(bookList, key=lambda e: (-e[3], -e[1], e[0]))\n",
    "\n",
    "print(f'Found {len(empty):>4} empty books')\n",
    "print(f'Found {len(ordinary):>4} ordinary books (i.e. without private words)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:18:52.143337Z",
     "start_time": "2018-05-18T09:18:52.130385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book                 #all #own %own\n",
      "-----------------------------------\n",
      "3                    4719 1099 23.3%\n",
      "14                   4777 1090 22.8%\n",
      "15                   3821  840 22.0%\n",
      "13                   4945 1078 21.8%\n",
      "11                   4589  997 21.7%\n",
      "7                    4457  948 21.3%\n",
      "5                    4068  831 20.4%\n",
      "4                    4777  955 20.0%\n",
      "2                    3787  741 19.6%\n",
      "9                    4109  797 19.4%\n",
      "1                    3715  692 18.6%\n",
      "6                    4453  824 18.5%\n",
      "10                   4333  751 17.3%\n",
      "12                   4165  712 17.1%\n",
      "8                    3488  544 15.6%\n",
      "...\n",
      "3                    4719 1099 23.3%\n",
      "14                   4777 1090 22.8%\n",
      "15                   3821  840 22.0%\n",
      "13                   4945 1078 21.8%\n",
      "11                   4589  997 21.7%\n",
      "7                    4457  948 21.3%\n",
      "5                    4068  831 20.4%\n",
      "4                    4777  955 20.0%\n",
      "2                    3787  741 19.6%\n",
      "9                    4109  797 19.4%\n",
      "1                    3715  692 18.6%\n",
      "6                    4453  824 18.5%\n",
      "10                   4333  751 17.3%\n",
      "12                   4165  712 17.1%\n",
      "8                    3488  544 15.6%\n"
     ]
    }
   ],
   "source": [
    "print('{:<20}{:>5}{:>5}{:>5}\\n{}'.format(\n",
    "    'book', '#all', '#own', '%own',\n",
    "    '-'*35,\n",
    "))\n",
    "\n",
    "for x in bookList[0:20]:\n",
    "  print('{:<20} {:>4} {:>4} {:>4.1f}%'.format(*x))\n",
    "print('...')\n",
    "for x in bookList[-20:]:\n",
    "  print('{:<20} {:>4} {:>4} {:>4.1f}%'.format(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
